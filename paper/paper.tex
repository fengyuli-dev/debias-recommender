% My template
\documentclass[12pt, letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[amsthm]{ntheorem}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{enumitem}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\widowpenalty10000
\clubpenalty10000
\setlength{\parskip}{\baselineskip}
\setlength{\parindent}{0pt}
\setlist[enumerate]{topsep=0pt}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}\newtheorem*{definition}{Definition}
\theoremstyle{definition}\newtheorem*{example}{Example}
\theoremstyle{definition}\newtheorem*{remark}{Remark}
\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\mb}[1]{\mathbf{#1}}
\def\RR{\mathbb{R}}
\def\ZZ{\mathbb{Z}}
\def\EE{\mathbb{E}}
\def\PP{\mathbb{P}}
\def\QQ{\mathbb{Q}}
\def\II{\mathbb{I}}
\def\CC{\mathbb{C}}
\def\NN{\mathbb{N}}
\def\OO{\mathbb{O}}
\def\HH{\mathbb{H}}
\def\Ll{\mathcal{L}}
\def\Mm{\mathcal{M}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{\vspace{-2.0cm}Mitigating the Effect of Sampling Bias on Recommender Systems through Quantization}
\author{Fengyu Li, Sarah Dean}
\date{}

\begin{document}
\maketitle

\begin{abstract}
    Placleholder.
\end{abstract}

\section{Introduction}

\section{Draft Area}
 (Consider using 2-dimensional Gaussian?)\\
Assume the (flattened) ground truth ratings is a standard Gaussian with differential entropy
\begin{align*} \HH(R\sim N(0, 1)) & = -\EE[\log R]                    \\ & = -\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \log \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \, dx \\
                                  & = \frac{1}{2}\log 2\pi e          \\
                                  & = 2.05
\end{align*}
We can show that quantization (discretization) loses information. Consider
a simple binary quantization that splits the ratings around the mean. Denote the quantized r.v. as $Q_1$.
\[ \HH(Q_1) = -2 \cdot \frac{1}{2} \log \frac{1}{2} = 1 \]
Consider another quantized r.v. $Q_2$ that divides the distribution four-fold based on the four quantiles.
\[ \HH(Q_2) = -4 \cdot \frac{1}{4} \log \frac{1}{4} = 2 \]
We can see that different quantization schemes loses different amount of information.
\[ \HH(R) > \HH(Q_2) > \HH(Q_1) \]

Next: discussion
\end{document}

