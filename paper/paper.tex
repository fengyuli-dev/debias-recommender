\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
% \usepackage{neurips_2022}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
% \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage{neurips_2022}
\bibliographystyle{plain}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}

\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\mb}[1]{\mathbf{#1}}
\def\RR{\mathbb{R}}
\def\ZZ{\mathbb{Z}}
\def\EE{\mathbb{E}}
\def\PP{\mathbb{P}}
\def\QQ{\mathbb{Q}}
\def\II{\mathbb{I}}
\def\CC{\mathbb{C}}
\def\NN{\mathbb{N}}
\def\OO{\mathbb{O}}
\def\HH{\mathbb{H}}
\def\Ll{\mathcal{L}}
\def\Mm{\mathcal{M}}
\DeclareMathOperator*{\argmin}{arg\,min}


\title{Cross-Dataset Recommender System for \\Overcoming Selection Bias}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Fengyu Li \\
  Cornell University\\
  \texttt{fl334@cornell.edu} \\
  \And
  Sarah Dean \\
  Cornell University \\
  \texttt{sdean@cornell.edu} \\
}


\begin{document}


\maketitle


\begin{abstract}
  The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on
  both the left- and right-hand margins. Use 10~point type, with a vertical
  spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered,
  bold, and in point size 12. Two line spaces precede the abstract. The abstract
  must be limited to one paragraph. The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on
  both the left- and right-hand margins. Use 10~point type, with a vertical
  spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered,
  bold, and in point size 12. Two line spaces precede the abstract. The abstract
  must be limited to one paragraph.
\end{abstract}

\section{Introduction}
Selection bias is one of the most prevalent sources of biases for recommender systems \cite{chen2020bias}. Selection bias happens when there is a pattern in the users' ratings that is unique to the training set. For example, in a recommender system for movies, users might mainly rate movies that are recommended to them, which is a small section of movies already tailored to the users' propensities. However, the environment the recommender system is deployed on contains all movies regardless of personal tastes. This discrepancy produces a misalignment between training and deployed settings, which is known as a distribution shift. Tackling the selection bias in a recommender dataset has been a constant challenge in designing recommender algorithms\cite{schnabel2016recommendations}.

When recommender systems are deployed in real-world platforms, it is arguably likely that the clients are able to collect rating-associated data from different source distributions. These data (or feedbacks) are either implicit or explicit \cite{aggarwal2016recommender}, while implicit data often comes in a highly-quantized, binary form and explicit data is usually less quantized. For example, a user's explicit rating of a movie is usually quantized to a number between 1 and 5, while the implicit feedback, such as if a user spontaneous searches for a movie, is often binary (more quantized.) We extend our hypothesis and argue that implicit data contains less or no selection bias compared to explicit data. This is because implicit data are often from users' spontaneous actions while explicit data are prejudiced toward the output of the recommender system in the previous feedback loop and users' innate biases.

In this paper, we attempt to take advantage of datasets from differently quantized sources. More specifically, we proposed a way to feed both a 5-quantized dataset and a binary dataset to any gradient-based recommender algorithm.  To ensure both datasets do not significantly lose their values in the presence of selection bias, we first examine the susceptibility to selection bias of differently quantized datasets from a single distribution. We design experiments under a simulated environment and shows that susceptibility to selection bias is not correlated with the way a dataset is quantized. 

Then, since a less-quantized dataset inherently contains more information than a more-quantized dataset \cite{widrow1996statistical} and thus is more suitable for training, we decided to use it as the training data of matrix factorization and use the more-quantized dataset for propensity scoring and deriving the inverse-probability-scoring (IPS) estimator \cite{thompson2012sampling} \cite{imbens2015causal}, a causal inference approach applicable to matrix completion-based recommender algorithms. In this way, our cross-dataset learning framework empowers existing recommender algorithms to make use of the more quantized, less biased data. We carried out experiment and found our method outperforming baselines by a significant margin.

% Quantization is a practical way to categorize an input from a continuous distribution to a discrete set of bins. We assume the ground truth of a user's rating is a continuous distribution over items because of the inherent impossibility of numerically expressing the feeling of enjoyment and excitement. We treat a recommender dataset as a noised quantization of the ground truth. In other words, users convert their feelings into one of the discrete choices when they rate an item. Recommender datasets are quantized into different degrees. For example, a typical movie rating dataset uses a 5-quantization. A tinder like/dislike dataset uses a 2-quantization.

\section{Related Work}
Prior works on overcoming selection bias-induced distribution shift fall

\section{Susceptibility to Selection Bias}

\subsection{Simulated Environment for Controlling Bias}

\subsection{Results}

\section{Cross-Dataset Propensity Estimation}

\[ \argmin_{V,W,A} \frac{1}{N}\left(\frac{(Y_{u,i}- (V_u^TW_i + A))^2}{P_{u,i}} \right) + c \Vert A \Vert^2\]
where $A = \{ b_u, b_i, \mu \}$ represents the standard bias parameters (offset), $\hat{Y} = V_u^TW_i + A$ is the predicted rating, and $c \Vert A \Vert^2$ is the regularizer.

Stochastic gradient descent (SGD) step for one sample rating $(i,j)$:
\[ b_u \gets b_u - \alpha(b_u - \frac{1}{P_{i,j}}(r_{i,j} - \hat{r}_{i,j})) \]
\[ b_i \gets b_i - \alpha(b_i - \frac{1}{P_{i,j}}(r_{i,j} - \hat{r}_{i,j})) \]
\[ V_i \gets V_i - \alpha(V_i - \frac{1}{P_{i,j}}W_j(r_{i,j} - \hat{r}_{i,j})) \]
\[ W_j \gets W_j - \alpha(W_j - \frac{1}{P_{i,j}}V_i(r_{i,j} - \hat{r}_{i,j})) \]
where $\alpha$ is the learning rate, which can be tuned independently for each parameter. In our experiments, we fix $\alpha$ across parameters.

\section{Experiment}
We designed experiments under simulated environments that verify the results introduced above. We first verify that more quantized dataset is more susceptible to selection bias and MNAR. We then runs our learning framework and compares it with multiple baselines.

\subsection{Dataset and Baselines}

\subsection{Results}

\section{Conclusion}

\section*{Acknowledgements}

\bibliography{refs} 

\section{Draft Area}
 (Consider using 2-dimensional Gaussian?)\\
Assume the (flattened) ground truth ratings is a standard Gaussian with differential entropy
\begin{align} \HH(R\sim N(0, 1))
   & = \frac{1}{2}\log 2\pi e \\
   & = 2.05
\end{align}
We can show that quantization (discretization) loses information. Consider
a simple binary quantization that splits the ratings around the mean. Denote the quantized r.v. as $Q_1$.
\[ \HH(Q_1) = -2 \cdot \frac{1}{2} \log \frac{1}{2} = 1 \]
Consider another quantized r.v. $Q_2$ that divides the distribution four-fold based on the four quantiles.
\[ \HH(Q_2) = -4 \cdot \frac{1}{4} \log \frac{1}{4} = 2 \]
We can see that different quantization schemes loses different amount of information.
\[ \HH(R) > \HH(Q_2) > \HH(Q_1) \]
Next, we can prove that selection bias causes information loss. Assuming the selection bias causes ratings to have unequal probability to be sampled, and higher ratings are more likely to be sampled. The distribution of the observed ratings would then be right-skewed. We prove that, in the binary case, this reduces the information loss. Let $p$ be the probability that the sampled rating is positive.
\[\HH = p\log p + (1-p) \log (1-p) \]
\[ \frac{d\HH}{dp} = 1 + \log p + \frac{1-p}{p-1} - \log(1-p) = 0 \]
which is optimized when $p = 0.5$. This result extends to all $n$-quantizations (can be proved using Lagrange multipliers.) See \url{http://pillowlab.princeton.edu/teaching/statneuro2018/slides/notes08_infotheory.pdf}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}