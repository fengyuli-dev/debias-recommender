% My template
\documentclass[12pt, letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[amsthm]{ntheorem}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{enumitem}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\setlist[enumerate]{topsep=0pt}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}\newtheorem*{definition}{Definition}
\theoremstyle{definition}\newtheorem*{example}{Example}
\theoremstyle{definition}\newtheorem*{remark}{Remark}
\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\mb}[1]{\mathbf{#1}}
\def\RR{\mathbb{R}}
\def\ZZ{\mathbb{Z}}
\def\EE{\mathbb{E}}
\def\PP{\mathbb{P}}
\def\QQ{\mathbb{Q}}
\def\II{\mathbb{I}}
\def\CC{\mathbb{C}}
\def\NN{\mathbb{N}}
\def\OO{\mathbb{O}}
\def\HH{\mathbb{H}}
\def\Ll{\mathcal{L}}
\def\Mm{\mathcal{M}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{\vspace{-2.0cm}Mitigating Selection Bias Distribution Shift on Recommender Systems through Quantization }
\author{Fengyu Li, Sarah Dean}
\date{}

\begin{document}
\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}

\section{Related Work}

\subsection{Biases in Recommender Systems}

\subsection{Distribution Shift}

\section{Theoretical Explanations}

\subsection{Quantization and Selection Bias as Entropy Loss}

\subsection{Closing the Gap}

\section{Experiment}

\subsection{Datasets and Algorithms}

\subsection{Controlling Bias}

\subsection{Results}

\section{Conclusion}

\section{Draft Area}
 (Consider using 2-dimensional Gaussian?)\\
Assume the (flattened) ground truth ratings is a standard Gaussian with differential entropy
\begin{align*} \HH(R\sim N(0, 1)) & = -\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \log \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \, dx \\
                                  & = \frac{1}{2}\log 2\pi e                                                                                            \\
                                  & = 2.05
\end{align*}
We can show that quantization (discretization) loses information. Consider
a simple binary quantization that splits the ratings around the mean. Denote the quantized r.v. as $Q_1$.
\[ \HH(Q_1) = -2 \cdot \frac{1}{2} \log \frac{1}{2} = 1 \]
Consider another quantized r.v. $Q_2$ that divides the distribution four-fold based on the four quantiles.
\[ \HH(Q_2) = -4 \cdot \frac{1}{4} \log \frac{1}{4} = 2 \]
We can see that different quantization schemes loses different amount of information.
\[ \HH(R) > \HH(Q_2) > \HH(Q_1) \]
Next, we can prove that selection bias causes information loss. Assuming the selection bias causes ratings to have unequal probability to be sampled, and higher ratings are more likely to be sampled. The distribution of the observed ratings would then be right-skewed. We prove that, in the binary case, this reduces the information loss. Let $p$ be the probability that the sampled rating is positive.
\[\HH = p\log p + (1-p) \log (1-p) \]
\[ \frac{d\HH}{dp} = 1 + \log p + \frac{1-p}{p-1} - \log(1-p) = 0 \]
which is optimized when $p = 0.5$. This result extends to all $n$-quantizations (can be proved using Lagrange multipliers.) See \url{http://pillowlab.princeton.edu/teaching/statneuro2018/slides/notes08_infotheory.pdf}.



\end{document}

