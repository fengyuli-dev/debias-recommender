\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
% \usepackage{neurips_2022}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
% \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\bibliographystyle{ksfh_nat}

\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\mb}[1]{\mathbf{#1}}
\def\RR{\mathbb{R}}
\def\ZZ{\mathbb{Z}}
\def\EE{\mathbb{E}}
\def\PP{\mathbb{P}}
\def\QQ{\mathbb{Q}}
\def\II{\mathbb{I}}
\def\CC{\mathbb{C}}
\def\NN{\mathbb{N}}
\def\OO{\mathbb{O}}
\def\HH{\mathbb{H}}
\def\Ll{\mathcal{L}}
\def\Mm{\mathcal{M}}
\DeclareMathOperator*{\argmin}{arg\,min}


\title{Cross-Dataset Recommender System for \\Overcoming Selection Bias}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Fengyu Li \\
  Cornell University\\
  \texttt{fl334@cornell.edu} \\
  \And
  Sarah Dean \\
  Cornell University \\
  \texttt{sdean@cornell.edu} \\
}


\begin{document}


\maketitle


\begin{abstract}
  The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on
  both the left- and right-hand margins. Use 10~point type, with a vertical
  spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered,
  bold, and in point size 12. Two line spaces precede the abstract. The abstract
  must be limited to one paragraph. The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on
  both the left- and right-hand margins. Use 10~point type, with a vertical
  spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered,
  bold, and in point size 12. Two line spaces precede the abstract. The abstract
  must be limited to one paragraph. testf
\end{abstract}

\section{Introduction}
Selection bias is one of the most prevalent sources of distribution shift for recommender systems. Selection bias happens when there is a pattern in the users' ratings that is unique to the training set, either implicitly or explicitly. For example, in a recommender system for movies, users might only watch and rate movies that they particularly like, while distribution the recommender system is deployed on contains all movies regardless of person al tastes. Tackling the selection bias in a recommender dataset has been a constant challenge in designing recommender algorithms

Quantization is a practical way to categorize an input from a continuous distribution to a discrete set of bins. We assume the ground truth of a user's rating is a continuous distribution over items because of the inherent impossibility of numerically expressing the feeling of enjoyment and excitement. We treat a recommender dataset as a noised quantization of the ground truth. In other words, users convert their feelings into one of the discrete choices when they rate an item. Recommender datasets are quantized into different degrees. For example, a typical movie rating dataset uses a 5-quantization. A tinder like/dislike dataset uses a 2-quantization.


We hypothesize that quantization and the effect of selection bias on the recommender system is positively correlated: the more quantized the dataset is (the fewer bins there are), the more severe the selection bias becomes in terms of the recommender system's performance. Therefore, a good-performing recommender system should prioritize training data that is less quantized. 

In reality, however, less quantized data might only represent a small subset of all the data one is capable of collecting. This is because less quantized data is typically explicit while more quantized data is typically implicit. That is to say, less quantized data would require a user explicitly rating an item, while more quantized data would be more easily gathered by observing the users' behaviors. For example, by observing the movie a user spontaneously chooses to see or if a user watches a movie's trailer to the end, we can easily create a 1-quantized or 2-quantized dataset, correspondingly, without the user paying any effort. In this paper, we propose a novel framework that utilizes these data. Our framework specifically exploits the fact that more quantized data is more susceptible to selection bias and treats it as an advantage that helps less quantized dataset better overcome the damage caused by selection bias.

The main contribution of this work is two-fold:
\begin{enumerate}
  \item We design experiments under a simulated environment and shows that susceptibility to selection bias is likely not correlated with the way a dataset is quantized.
  \item We propose a cross-dataset learning framework that empowers existing recommender algorithms to make use of the more quantized, less biased data. We carried out experiment and found our method outperforming baselines by a significant margin.
\end{enumerate}

\section{Related Work}

\section{Susceptibility to Selection Bias}

\subsection{Simulated Environment for Controlling Bias}

\subsection{Results}
\bibliography{refs}

\section{Cross-Dataset Propensity Estimation}

\[ \argmin_{V,W,A} \frac{1}{N}\left(\frac{(Y_{u,i}- (V_u^TW_i + A))^2}{P_{u,i}} \right) + c \Vert A \Vert^2\]
where $A = \{ b_u, b_i, \mu \}$ represents the standard bias parameters (offset), $\hat{Y} = V_u^TW_i + A$ is the predicted rating, and $c \Vert A \Vert^2$ is the regularizer.

Stochastic gradient descent (SGD) step for one sample rating $(i,j)$:
\[ b_u \gets b_u - \alpha(b_u - \frac{1}{P_{i,j}}(r_{i,j} - \hat{r}_{i,j})) \]
\[ b_i \gets b_i - \alpha(b_i - \frac{1}{P_{i,j}}(r_{i,j} - \hat{r}_{i,j})) \]
\[ V_i \gets V_i - \alpha(V_i - \frac{1}{P_{i,j}}W_j(r_{i,j} - \hat{r}_{i,j})) \]
\[ W_j \gets W_j - \alpha(W_j - \frac{1}{P_{i,j}}V_i(r_{i,j} - \hat{r}_{i,j})) \]
where $\alpha$ is the learning rate, which can be tuned independently for each parameter. In our experiments, we fix $\alpha$ across parameters.

\section{Experiment}
We designed experiments under simulated environments that verify the results introduced above. We first verify that more quantized dataset is more susceptible to selection bias and MNAR. We then runs our learning framework and compares it with multiple baselines.

\subsection{Dataset and Baselines}

\subsection{Results}

\section{Conclusion}

\section*{Acknowledgements}

\section*{References}
\bibliography{refs}

\section{Draft Area}
 (Consider using 2-dimensional Gaussian?)\\
Assume the (flattened) ground truth ratings is a standard Gaussian with differential entropy
\begin{align} \HH(R\sim N(0, 1))
   & = \frac{1}{2}\log 2\pi e \\
   & = 2.05
\end{align}
We can show that quantization (discretization) loses information. Consider
a simple binary quantization that splits the ratings around the mean. Denote the quantized r.v. as $Q_1$.
\[ \HH(Q_1) = -2 \cdot \frac{1}{2} \log \frac{1}{2} = 1 \]
Consider another quantized r.v. $Q_2$ that divides the distribution four-fold based on the four quantiles.
\[ \HH(Q_2) = -4 \cdot \frac{1}{4} \log \frac{1}{4} = 2 \]
We can see that different quantization schemes loses different amount of information.
\[ \HH(R) > \HH(Q_2) > \HH(Q_1) \]
Next, we can prove that selection bias causes information loss. Assuming the selection bias causes ratings to have unequal probability to be sampled, and higher ratings are more likely to be sampled. The distribution of the observed ratings would then be right-skewed. We prove that, in the binary case, this reduces the information loss. Let $p$ be the probability that the sampled rating is positive.
\[\HH = p\log p + (1-p) \log (1-p) \]
\[ \frac{d\HH}{dp} = 1 + \log p + \frac{1-p}{p-1} - \log(1-p) = 0 \]
which is optimized when $p = 0.5$. This result extends to all $n$-quantizations (can be proved using Lagrange multipliers.) See \url{http://pillowlab.princeton.edu/teaching/statneuro2018/slides/notes08_infotheory.pdf}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}